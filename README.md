# ü§ñ Agente Conversacional de PDFs com RAG e LLM

Este projeto implementa um agente conversacional capaz de "ler" e responder perguntas sobre o conte√∫do de documentos PDF. Ele utiliza a arquitetura RAG (Retrieval Augmented Generation) com Modelos de Linguagem Amplos (LLMs) para fornecer respostas contextuais e precisas, baseadas exclusivamente nas informa√ß√µes presentes no documento fornecido.

## ‚ú® Funcionalidades

* **Upload de Documentos PDF:** Interface amig√°vel para enviar arquivos PDF.
* **Processamento Inteligente:** Extra√ß√£o de texto, divis√£o em chunks sem√¢nticos e gera√ß√£o de embeddings.
* **Armazenamento Vetorial:** Indexa√ß√£o dos chunks para busca r√°pida por similaridade (por padr√£o, opera em mem√≥ria para cada sess√£o de PDF, mas pode ser configurado para persist√™ncia).
* **Interface de Chat Interativa:** Converse com o agente sobre o conte√∫do do PDF carregado.
* **Respostas Contextualizadas (RAG):** O LLM responde perguntas utilizando apenas o contexto extra√≠do do PDF.
* **Suporte Flex√≠vel a LLMs:**
    * Integrado com LLMs locais via **Ollama** (ex: Qwen, Mistral, Llama).
    * Facilmente configur√°vel para usar LLMs via API (ex: Google Gemini, OpenAI GPT).
* **Modelo de Embedding Configur√°vel:** Suporte para modelos de embedding locais (via HuggingFace Sentence Transformers) ou via API.

## üèõÔ∏è Arquitetura Simplificada

A aplica√ß√£o segue um fluxo RAG:

1.  **Interface do Usu√°rio (Streamlit):** O usu√°rio faz upload do PDF e interage via chat.
2.  **Processador de PDF (`PDFProcessor`):** Carrega o PDF, extrai o texto e o divide em chunks.
3.  **Gera√ß√£o de Embeddings (`LLMHandler`):** Transforma os chunks de texto em vetores num√©ricos (embeddings).
4.  **Banco de Dados Vetorial (`VectorStore` - ChromaDB):** Armazena os chunks e seus embeddings. Para cada PDF processado, um novo banco vetorial em mem√≥ria √© criado por padr√£o, garantindo que as conversas sejam isoladas por documento e sess√£o.
5.  **Pipeline RAG (`RAGPipeline`):**
    * Quando o usu√°rio faz uma pergunta:
        * A pergunta √© convertida em um embedding.
        * O `Retriever` busca os chunks mais relevantes no banco vetorial.
        * Os chunks relevantes (contexto) e a pergunta s√£o enviados para o LLM (via `LLMHandler`).
    * O LLM gera uma resposta baseada no contexto fornecido.
6.  **LLM (`LLMHandler`):** Modelo de Linguagem Amplo que gera as respostas.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python 3.9+
* **Interface:** Streamlit
* **Orquestra√ß√£o RAG e LLM:** LangChain
* **LLMs Locais:** Ollama (permite rodar modelos como Qwen, Mistral, Llama 3, etc.)
* **LLMs via API (Opcional):** Google Gemini, OpenAI GPT
* **Embeddings:** HuggingFace Sentence Transformers (padr√£o, local), Google Gemini Embeddings, OpenAI Embeddings
* **Banco de Dados Vetorial:** ChromaDB (operando em mem√≥ria por padr√£o para cada PDF)
* **Processamento de PDF:** PyMuPDF

## ‚öôÔ∏è Pr√©-requisitos

* Python (vers√£o 3.9 ou superior recomendada)
* Git
* **Opcional (para LLMs locais):** Ollama instalado e configurado.
    * Certifique-se de ter baixado os modelos que deseja usar (ex: `ollama pull qwen:1.8b`, `ollama pull mistral`).
* **Opcional (para acelera√ß√£o de GPU com Ollama):** Drivers NVIDIA atualizados e CUDA (para GPUs NVIDIA) ou configura√ß√£o apropriada para GPUs AMD/Apple Silicon.

## üöÄ Configura√ß√£o e Instala√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO_AQUI>
    cd nome-do-diretorio-do-projeto
    ```

2.  **Crie e Ative um Ambiente Virtual:**
    ```bash
    python -m venv .venv
    ```
    * No Linux/macOS:
        ```bash
        source .venv/bin/activate
        ```
    * No Windows (PowerShell):
        ```bash
        .\.venv\Scripts\Activate.ps1
        ```
    * No Windows (CMD):
        ```bash
        .\.venv\Scripts\activate.bat
        ```

3.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```
    *Nota: Se voc√™ encontrar um erro relacionado ao `protobuf` durante a instala√ß√£o ou execu√ß√£o, tente:*
    ```bash
    pip install protobuf==3.20.3
    ```

4.  **Configure as Vari√°veis de Ambiente (Opcional):**
    * Se voc√™ planeja usar LLMs ou modelos de embedding via API (Google Gemini, OpenAI), crie um arquivo chamado `.env` na raiz do projeto.
    * Copie o conte√∫do de `.env.example` (se voc√™ criar um) para `.env` e preencha suas chaves de API:
      ```env
      # .env
      GOOGLE_API_KEY="sua_chave_api_google_aqui"
      OPENAI_API_KEY="sua_chave_api_openai_aqui"
      ```
    * A configura√ß√£o padr√£o no `core/rag_pipeline.py` utiliza embeddings locais (HuggingFace) e tenta usar Google Gemini para o LLM. Se `GOOGLE_API_KEY` n√£o estiver definida, a chamada ao LLM falhar√°. Voc√™ pode ajustar `DEFAULT_RAG_CONFIG` em `core/rag_pipeline.py` para usar Ollama como provedor de LLM por padr√£o, se preferir.

## ‚ñ∂Ô∏è Executando a Aplica√ß√£o

1.  **Inicie o Servidor Ollama (se estiver usando LLMs locais e ele n√£o estiver rodando como servi√ßo):**
    Abra um terminal e execute:
    ```bash
    ollama serve
    ```
    * Certifique-se de que o modelo que voc√™ pretende usar com Ollama (ex: `qwen:1.8b`) j√° foi baixado (`ollama pull nome_do_modelo`).

2.  **Execute a Aplica√ß√£o Streamlit:**
    No terminal (com o ambiente virtual ativado), na pasta raiz do projeto, execute:
    ```bash
    streamlit run app.py
    ```
    A aplica√ß√£o dever√° abrir automaticamente no seu navegador web.

## üìñ Como Usar

1.  Acesse a interface no navegador (geralmente `http://localhost:8501`).
2.  Na barra lateral esquerda, clique em "Escolha um arquivo PDF" e selecione o documento PDF que deseja analisar.
3.  Aguarde o processamento do PDF. Voc√™ ver√° mensagens de status.
4.  Uma vez que o PDF esteja processado, a caixa de chat ser√° habilitada. Digite sua pergunta sobre o conte√∫do do documento e pressione Enter.
5.  O agente responder√° com base nas informa√ß√µes encontradas no PDF.

## üìÅ Estrutura do Projeto (Vis√£o Geral)
meu_agente_pdf/
‚îú‚îÄ‚îÄ app.py                   # C√≥digo da interface Streamlit
‚îú‚îÄ‚îÄ core/                    # L√≥gica principal da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ init.py
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py     # Carregamento e divis√£o de PDFs
‚îÇ   ‚îú‚îÄ‚îÄ llm_handler.py       # Gerenciamento de LLMs e modelos de embedding
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py      # Gerenciamento do banco de dados vetorial (ChromaDB)
‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py      # Orquestra√ß√£o do pipeline RAG
‚îú‚îÄ‚îÄ data/                    # Dados gerados ou tempor√°rios
‚îÇ   ‚îú‚îÄ‚îÄ pdf_uploads_temp/    # Armazenamento tempor√°rio de PDFs enviados
‚îÇ   ‚îî‚îÄ‚îÄ vector_stores_persistent/ # (Opcional) Para vector stores persistentes, se configurado
‚îú‚îÄ‚îÄ .env                     # (Local, n√£o versionado) Chaves de API e outras configura√ß√µes secretas
‚îú‚îÄ‚îÄ requirements.txt         # Lista de depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                # Este arquivo
